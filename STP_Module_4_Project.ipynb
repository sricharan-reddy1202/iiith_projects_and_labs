{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sricharan-reddy1202/iiith_projects_and_labs/blob/main/STP_Module_4_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 4: Linear Classifiers & Gradient Descent"
      ],
      "metadata": {
        "id": "xET1TacFQWQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case Study: Predictive Modeling for Public Water Safety**\n",
        "\n",
        "**Objective:** Develop a robust classifier to identify potable water samples. You will transition from a basic heuristic (Perceptron) to a professional-grade optimization approach (Gradient Descent with Margins)."
      ],
      "metadata": {
        "id": "_V7gSkZHQXdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data Acquisition & Cleaning\n",
        "\n",
        "In real-world data science, datasets are rarely perfect. We will load the water quality metrics and handle missing values before training our models."
      ],
      "metadata": {
        "id": "YjRDChliR1BK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset from a public raw GitHub URL\n",
        "url = \"https://raw.githubusercontent.com/nferran/tp_aprendizaje_de_maquina_I/main/water_potability.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 1: Handling Missing Values\n",
        "# Water sensors often fail, leaving NaNs. We will fill them with the mean of the column.\n",
        "df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "# Step 2: Feature Selection & Labeling\n",
        "# We'll use all chemical features to predict 'Potability'\n",
        "X = df.drop('Potability', axis=1).values\n",
        "y = df['Potability'].values\n",
        "\n",
        "# Step 3: Class Label Conversion\n",
        "# Many linear classifiers (like Perceptron/SVM) require labels to be -1 and 1\n",
        "y = np.where(y == 0, -1, 1)\n",
        "\n",
        "# Step 4: Train-Test Split & Scaling\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Dataset Loaded: {X_train.shape[0]} training samples, {X_train.shape[1]} features.\")"
      ],
      "metadata": {
        "id": "LOg9j_w7R8uU",
        "outputId": "5463a397-2a10-4b7b-df3f-42c0c0e6eef5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Loaded: 2620 training samples, 9 features.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Phase 1: The Heuristic Approach (Perceptron)\n",
        "\n",
        "The **Perceptron** represents the earliest form of supervised learning. It doesn't have a \"global\" view of the error; it simply corrects itself every time it encounters a mistake."
      ],
      "metadata": {
        "id": "_Mr2gopTSBjh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task:** Implement the Perceptron Update Rule inside the training loop."
      ],
      "metadata": {
        "id": "h16OQo9LSLDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WaterPerceptron:\n",
        "    def __init__(self, lr=0.01, epochs=50):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.w = None\n",
        "        self.b = 0\n",
        "        self.mistakes = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.w = np.zeros(X.shape[1])\n",
        "        for epoch in range(self.epochs):\n",
        "            count = 0\n",
        "            for i in range(len(y)):\n",
        "                # TODO: Calculate the linear output (w * x + b)\n",
        "                # prediction = ...\n",
        "\n",
        "                # TODO: If prediction is a mistake (y * prediction <= 0):\n",
        "                # Update weights: w = w + lr * y * x\n",
        "                # Update bias: b = b + lr * y\n",
        "                pass # remove this\n",
        "            self.mistakes.append(count)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w) + self.b)\n",
        "\n",
        "# model_p = WaterPerceptron()\n",
        "# model_p.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "mqxF5SQGSA8-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Phase 2: Gradient Descent - Global Optimization\n",
        "\n",
        "The Perceptron is unstable if the data isn't perfectly separable. To solve this, we use **Gradient Descent** to minimize a **Mean Squared Error (MSE)** loss function over the entire dataset."
      ],
      "metadata": {
        "id": "Xzzq_ziOSQmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task:** Implement the batch gradient calculation for weights and bias."
      ],
      "metadata": {
        "id": "xJzJuR77SRiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GDWaterClassifier:\n",
        "    def __init__(self, lr=0.001, epochs=500):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.w = None\n",
        "        self.b = 0\n",
        "        self.cost_history = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.w = np.zeros(X.shape[1])\n",
        "        n = X.shape[0]\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            # TODO: 1. Compute linear output: z = Xw + b\n",
        "            # TODO: 2. Calculate gradients:\n",
        "            # dw = (1/n) * X.T.dot(z - y)\n",
        "            # db = (1/n) * sum(z - y)\n",
        "\n",
        "            # TODO: 3. Update w and b: w = w - lr * dw\n",
        "            pass\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w) + self.b)"
      ],
      "metadata": {
        "id": "Q9iFCaG3Se2L"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Phase 3: Margin Classifiers & Hinge Loss\n",
        "\n",
        "In water safety, we aim for more than just correctness—we want a **Margin**, a safety gap between safe and unsafe samples. This is achieved using **Hinge Loss** combined with **L2 Regularization**.\n",
        "\n",
        "The loss function is defined as:\n",
        "\n",
        "$$\n",
        "\\text{Loss} = \\lambda \\|w\\|^2_2 + \\sum_{i} \\max(0, 1 - y_i (w^T x_i + b))\n",
        "$$\n",
        "\n",
        "### Key Components:\n",
        "- **Hinge Loss**: $\\max(0, 1 - y_i (w^T x_i + b))$ ensures correct classification with a margin.\n",
        "- **L2 Regularization**: $\\lambda \\|w\\|^2_2$ penalizes large weights, promoting generalization and stability.\n"
      ],
      "metadata": {
        "id": "xT9CDlzUSf65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MarginWaterClassifier:\n",
        "    def __init__(self, lr=0.001, lambda_param=0.01, epochs=500):\n",
        "        self.lr = lr\n",
        "        self.lambda_param = lambda_param\n",
        "        self.epochs = epochs\n",
        "        self.w = None\n",
        "        self.b = 0\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.w = np.zeros(X.shape[1])\n",
        "        for _ in range(self.epochs):\n",
        "            for i, x_i in enumerate(X):\n",
        "                # TODO: Implement the Margin Condition check: y_i * (w * x_i + b) >= 1\n",
        "                if False: # Replace False with condition\n",
        "                    # Only Regularization update\n",
        "                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n",
        "                else:\n",
        "                    # Update for weight (including Hinge Loss) and bias\n",
        "                    # self.w -= self.lr * (2 * self.lambda_param * self.w - x_i * y[i])\n",
        "                    # self.b -= self.lr * (-y[i])\n",
        "                    pass\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w) + self.b)"
      ],
      "metadata": {
        "id": "RSLdAztpS03K"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Critical Analysis & Comparison\n",
        "\n",
        "**Analysis Tasks:**\n",
        "1. Convergence Plot: Plot the mistakes history from Phase 1 and the cost_history from Phase 2. Discuss why the Gradient Descent plot is smoother.\n",
        "2. Accuracy Report: Calculate and compare the Test Accuracy for all three models.\n",
        "3. Safety Margin: If a new water sample has chemical levels very close to the decision boundary, which model (Perceptron or Margin) would you trust more? Why?"
      ],
      "metadata": {
        "id": "VOxVmUrBS64Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion Questions\n",
        "\n",
        "### Q1: Impact of High Learning Rate in Gradient Descent\n",
        "What happens to your **Gradient Descent** model if you set the `learning_rate` too high (e.g., `1.0`)?\n",
        "*Hint: Think about convergence, overshooting, and divergence.*\n",
        "\n",
        "---\n",
        "\n",
        "### Q2: Label Conversion in Classification\n",
        "Why did we convert the labels to **$\\{-1, 1\\}$** instead of keeping them as **$\\{0, 1\\}$**?\n",
        "*Hint: Consider the mathematical formulation of the loss function (e.g., Hinge Loss) and symmetry.*\n",
        "\n",
        "---\n",
        "\n",
        "### Q3: Handling Noisy Data (Water Potability Dataset)\n",
        "The **Water Potability dataset** is often \"noisy\" (not perfectly separable). Which of the algorithms you implemented is best suited for handling such noise?\n",
        "*Hint: Think about robustness to outliers and margin-based classifiers.*\n"
      ],
      "metadata": {
        "id": "jgYdUvAvTDxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans 1\n",
        "If the learning rate is too high, gradient descent takes very large steps.\n",
        "This causes the model to overshoot the minimum and start oscillating.\n",
        "As a result, the algorithm may diverge and fail to converge.\n",
        "\n",
        "Ans 2\n",
        "Labels are converted to {−1,1} to maintain symmetry in mathematical formulas.\n",
        "Loss functions like hinge loss work correctly only with ±1 labels.\n",
        "This makes optimization simpler and more efficient.\n",
        "\n",
        "Ans 3\n",
        "Soft-margin SVM is best suited for noisy and non-separable data.\n",
        "It allows some misclassifications while still maximizing the margin.\n",
        "This makes it more robust to noise and outliers.\n",
        "\n"
      ],
      "metadata": {
        "id": "1q_mvz6eQeRa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pkVVBBtqQfwb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IXW0VDNCNF3T"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}